{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9af91b78-b2bb-4c16-b853-4dcaa6f28b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta, date\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c1a842-6e29-477d-baaa-1569f76fff93",
   "metadata": {},
   "source": [
    "# Check that MPS is available\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd9c3e8e-cf01-4757-8b6b-cb937c585169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff89a1d8-ec99-4922-8e5e-70ccdc470b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n"
     ]
    }
   ],
   "source": [
    "class CompactAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CompactAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(10, 16, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.ReLU(True),\n",
    "            #nn.Conv3d(8, 16, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # Bottleneck (no further reduction of dimensions)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose3d(16, 1, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.ReLU(True),\n",
    "            #nn.ConvTranspose3d(8, 1, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Create autoencoder model and use all available GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    autoencoder = nn.DataParallel(CompactAutoencoder()).to(device)\n",
    "else:\n",
    "    autoencoder = CompactAutoencoder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "513fcced-f2d8-46ce-96e3-3a667aff169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetCDFDataset(Dataset):\n",
    "    def __init__(self, root_dir, start_date, end_date, transform=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.file_list = self.create_file_list(root_dir, start_date, end_date)\n",
    "        self.mean = 278.83097 #279.9124\n",
    "        self.std = 56.02780 #107.1107\n",
    "        self.transform = transform\n",
    "\n",
    "    @staticmethod\n",
    "    def create_file_list(root_dir, start_date, end_date):\n",
    "        file_list = []\n",
    "        time_step = timedelta(days=1)\n",
    "        current_date = start_date\n",
    "\n",
    "        while current_date <= end_date:\n",
    "            for hh in ['00', '06', '12', '18']:\n",
    "                filename = f'{os.path.basename(root_dir)}.{current_date.strftime(\"%Y%m%d\")}{hh}.nc'\n",
    "                file_list.append(filename)\n",
    "            current_date += time_step\n",
    "        \n",
    "        return file_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.root_dir, self.file_list[idx])\n",
    "        \n",
    "        # Load NetCDF data\n",
    "        dataset = xr.open_dataset(file_path)\n",
    "        \n",
    "        # Get a list of all variable names in the dataset\n",
    "        variable_names = dataset.data_vars\n",
    "        \n",
    "        all_data = np.zeros((len(variable_names), 129, 181, 360), dtype=np.float32)\n",
    "        \n",
    "        for idx, var_name in enumerate(variable_names):\n",
    "            data = dataset.variables[var_name][:].astype(np.float32)\n",
    "            data = data.fillna(0)\n",
    "            all_data[idx, :, :, :] = data  # Fill the array with data for each variable\n",
    "\n",
    "        dataset.close()\n",
    "\n",
    "        # data = dataset.variables['t2m'][:].astype(np.float32)  # Adjust 'data' to the variable name in your file\n",
    "        # dataset.close()\n",
    "        \n",
    "        # Reshape the data to (1, 50, 721, 1440)\n",
    "        # data = data.reshape(1, 50, 721, 1440)\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.normalize_data(data)  # Normalize the data if transform is True\n",
    "\n",
    "        return torch.tensor(data)\n",
    "\n",
    "    def normalize_data(self, data):\n",
    "        data = (data - self.mean) / self.std\n",
    "        return data\n",
    "\n",
    "    def rescale_data(self, data):\n",
    "        data = (data * self.std) + self.mean\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b7badb-74c0-4af9-86c6-e14f66316170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_files(start_date, end_date, gfs_directory, era5_directory):\n",
    "    time_step = timedelta(days=1)\n",
    "    current_date = start_date\n",
    "    total_missing_files = 0\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime(\"%Y%m%d\")\n",
    "        for hour_str in ['00', '06', '12', '18']:\n",
    "            gfs_file_name = f\"GFS.{date_str}{hour_str}.nc\"\n",
    "            gfs_file_path = os.path.join(gfs_directory, gfs_file_name)\n",
    "\n",
    "            era5_file_name = f\"ERA5.{date_str}{hour_str}.nc\"\n",
    "            era5_file_path = os.path.join(era5_directory, era5_file_name)\n",
    "\n",
    "            if not os.path.exists(gfs_file_path):\n",
    "                print(f\"Missing file in GFS directory: {gfs_file_name}\")\n",
    "                total_missing_files += 1\n",
    "\n",
    "            if not os.path.exists(era5_file_path):\n",
    "                print(f\"Missing file in ERA5 directory: {era5_file_name}\")\n",
    "                total_missing_files += 1\n",
    "\n",
    "        current_date += time_step\n",
    "\n",
    "    print(f\"Total number of missing files: {total_missing_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "070b1432-f3d0-432b-9d5b-9103f39ca5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_and_std(root_dir, start_date, end_date):\n",
    "    time_step = timedelta(days=1)\n",
    "    current_date = start_date\n",
    "    total_count = 0\n",
    "    total_mean = 0.0\n",
    "    total_var = 0.0\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        for hour in ['00', '06', '12', '18']:\n",
    "            filename = f\"GFS.t2m.{current_date.strftime('%Y%m%d')}{hour}.nc\"\n",
    "            file_path = os.path.join(root_dir, filename)\n",
    "\n",
    "            if os.path.exists(file_path):\n",
    "                dataset = nc.Dataset(file_path)\n",
    "                data = dataset.variables['t2m'][:]  # Adjust this to your variable name\n",
    "                dataset.close()\n",
    "\n",
    "                current_mean = np.mean(data)\n",
    "                total_mean = (total_count * total_mean + len(data) * current_mean) / (total_count + len(data))\n",
    "                total_var = (total_count * total_var + np.sum((data - current_mean) ** 2)) / (total_count + len(data))\n",
    "                total_count += len(data)\n",
    "\n",
    "        current_date += time_step\n",
    "\n",
    "    total_std = np.sqrt(total_var / total_count)\n",
    "    return total_mean, total_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c23e95cd-d007-4bbd-81c3-ad73009ac065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of missing files: 0\n"
     ]
    }
   ],
   "source": [
    "# Define your data directories\n",
    "gfs_root_dir = '../data/GFS'\n",
    "era5_root_dir = '../data/ERA5'\n",
    "\n",
    "# Define the start and end date for the dataset\n",
    "start_date = date(2021, 3, 23)  # Adjust the start date\n",
    "end_date = date(2023, 3, 23)    # Adjust the end date\n",
    "\n",
    "check_missing_files(start_date, end_date, gfs_root_dir, era5_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ed1fbb-31b9-46d3-9119-ef752a94a86f",
   "metadata": {},
   "source": [
    "mean_value, std_value = calculate_mean_and_std(gfs_root_dir, start_date, end_date)\n",
    "print(f\"Mean: {mean_value}, Standard Deviation: {std_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e1e2865-b52c-4211-aba9-cab30c20a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GFS and ERA5 datasets\n",
    "gfs_dataset = NetCDFDataset(gfs_root_dir, start_date, end_date)\n",
    "era5_dataset = NetCDFDataset(era5_root_dir, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7915199-0d5d-4b73-a66c-292561e06c01",
   "metadata": {},
   "source": [
    "gfs_dataset.file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e1736b0-02cd-43e4-b87c-6c1bca3830f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the shuffled indices for both datasets\n",
    "shuffled_indices = torch.randperm(len(gfs_dataset))\n",
    "\n",
    "# Apply shuffled indices to both datasets\n",
    "gfs_dataset.file_list = [gfs_dataset.file_list[i] for i in shuffled_indices]\n",
    "era5_dataset.file_list = [era5_dataset.file_list[i] for i in shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62497b12-7169-49f5-9262-4e7e8f167256",
   "metadata": {},
   "source": [
    "era5_dataset.file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90441df1-5e5b-4aee-9a76-cd97043610d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "shuffle = False\n",
    "num_workers = 0\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.0001)\n",
    "\n",
    "# print(gfs_dataset)\n",
    "gfs_data_loader = DataLoader(gfs_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "era5_data_loader = DataLoader(era5_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127433a-2721-47de-8d07-fb556b35cf64",
   "metadata": {},
   "source": [
    "# Access the dataset from the DataLoader\n",
    "dataset = gfs_data_loader.dataset\n",
    "\n",
    "# Retrieve the file list from the dataset\n",
    "file_list = dataset.file_list  # Access the file list attribute (change 'file_list' to your dataset attribute)\n",
    "print(file_list)  # Display the file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff2bfae-caab-45d8-ac8d-58b3a031a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with a custom progress bar\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    autoencoder.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Create a custom progress bar for the epoch\n",
    "    progress_bar = tqdm(enumerate(zip(gfs_data_loader, era5_data_loader)), total=len(gfs_data_loader), desc=f'Epoch [{epoch+1}/{num_epochs}]', dynamic_ncols=True)\n",
    "    for batch_idx, (gfs_data, era5_data) in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(gfs_data.to(device))\n",
    "        loss = criterion(outputs, era5_data.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    progress_bar.close()  # Close the custom progress bar\n",
    "\n",
    "    # Calculate and print the average loss for the epoch\n",
    "    avg_loss = total_loss / len(gfs_data_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(autoencoder.module.state_dict() if isinstance(autoencoder, nn.DataParallel) else autoencoder.state_dict(), f'autoencoder_model_epoch_{epoch+1}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A100",
   "language": "python",
   "name": "a100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
